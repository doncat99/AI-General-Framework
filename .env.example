##############
# .env file (example)
##############

# =========================
# Service / API Meta
# =========================
API_VERSION=1.6.0
LOG_LEVEL=DEBUG               # DEBUG, INFO, WARNING, ERROR
DEBUG=1

# =========================
# Security / CORS
# =========================
SECURITY__JWT_SECRET_KEY=DVnFmhwvjEhJZpuhndxjhlezxQPJmBIIkMDEmFREWQADPcUnrG
SECURITY__BACKEND_CORS_ORIGINS=["http://localhost:3000","http://localhost:8001","http://localhost:8000"]
SECURITY__ALLOWED_HOSTS=["localhost","127.0.0.1"]

# =========================
# App & Service Ports
# =========================
APP_PORT=8000
MEM0_PORT=8001
NEO4J_HTTP_PORT=7474
NEO4J_BOLT_PORT=7687
QDRANT_HTTP_PORT=6333
QDRANT_GRPC_PORT=6334
OLLAMA_PORT=11434

# =========================
# Primary App Database
# =========================
DATABASE__HOSTNAME=app-postgres
DATABASE__USERNAME=postgres
DATABASE__PASSWORD=123
DATABASE__PORT=5433
DATABASE__DB=universe

# =========================
# Neo4j
# =========================
NEO4J_URI=bolt://localhost:${NEO4J_BOLT_PORT}
NEO4J_USER=neo4j
NEO4J_PASSWORD=neo4j_local_dev_password   # CHANGEME
NEO4J_DB=neo4j

# =========================
# Langfuse
# =========================
LANGFUSE_PUBLIC_KEY=<LANGFUSE_PUBLIC_KEY>  # CHANGEME
LANGFUSE_SECRET_KEY=<LANGFUSE_SECRET_KEY>  # CHANGEME
LANGFUSE_HOST=http://localhost:3000

# --- OTLP → Langfuse Worker ---
# OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://localhost:3030/v1/traces
# OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
# OTEL_EXPORTER_OTLP_HEADERS="x-langfuse-public-key=${LANGFUSE_PUBLIC_KEY},x-langfuse-secret-key=${LANGFUSE_SECRET_KEY}"
# OTEL_RESOURCE_ATTRIBUTES="service.name=ontosynth,service.version=dev,deployment.environment=local"

# =========================
# Langfuse backing services
# =========================
POSTGRES_VERSION=16
POSTGRES_USER=langfuse
POSTGRES_PASSWORD=langfusepass
POSTGRES_DB=langfuse
POSTGRES_PORT=5432
POSTGRES_HOST=langfuse-postgres

REDIS_HOST=langfuse-redis
REDIS_PORT=6379
REDIS_AUTH=myredissecret

MINIO_ROOT_USER=minio
MINIO_ROOT_PASSWORD=miniosecret

CLICKHOUSE_USER=clickhouse
CLICKHOUSE_PASSWORD=clickhouse

# =========================
# LLMs & APIs
# =========================
# OpenRouter
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
OPENROUTER_API_KEY=<OPENROUTER_API_KEY>    # CHANGEME

# OpenAI (optional; many routes go via OpenRouter)
OPENAI_BASE_URL=<OPENAI_BASE_URL>
OPENAI_API_KEY=<OPENAI_API_KEY>

# Google
GEMINI_API_KEY=<GEMINI_API_KEY>
LANGEXTRACT_API_KEY=<LANGEXTRACT_API_KEY>

# Anthropic
ANTHROPIC_API_KEY=<ANTHROPIC_API_KEY>

# Mistral
MISTRAL_API_KEY=<MISTRAL_API_KEY>

# Cohere
COHERE_API_KEY=<COHERE_API_KEY>

# DeepSeek
DEEPSEEK_API_BASE=https://api.deepseek.com
DEEPSEEK_API_KEY=<DEEPSEEK_API_KEY>

# Hugging Face
HUGGINGFACE_TOKEN=<huggingface_token>

# Stability
STABILITY_HOST=grpc.stability.ai:443
STABILITY_KEY=<STABILITY_KEY>             # CHANGEME

# =========================
# Vector DB (Qdrant)
# =========================
QDRANT_URL=http://qdrant:${QDRANT_HTTP_PORT}
QDRANT_API_KEY=<QDRANT_API_KEY>

# =========================
# Chroma (embedded, shared with mem0)
# =========================
# Primary path used by mem0’s vector_store (inside mem0 container).
# Keep this in sync with mem0 service’s env & volume in docker-compose.
CHROMA_PATH=/app/data/chroma_db
MEM0_COLLECTION=rag_memories

# =========================
# Ollama (containerized)
# =========================
# Inside containers, use the service name; from host, use http://localhost:11434
OLLAMA_HOST=http://ollama:${OLLAMA_PORT}
OLLAMA_BASE_URL=http://ollama:${OLLAMA_PORT}
OLLAMA_API_BASE=http://ollama:${OLLAMA_PORT}
# Embeddings model for mem0 + retriever:
OLLAMA_EMBED_MODEL=nomic-embed-text

# =========================
# Mem0 (server URL + internal paths)
# =========================
# Host → mem0 via published port (compose maps ${MEM0_PORT}:8000)
# The app/CLI uses this; containers generally call http://mem0:8000
MEM0_SERVER_URL=http://127.0.0.1:${MEM0_PORT}

MEM0_DATA_DIR=/app/data
MEM0_CONFIG_DIR=/app/config

# =========================
# Optional: Tracing / Assistant toggles
# =========================
# Disable OpenInference exporter to silence 401s during local dev
OPENINFERENCE_DISABLED=0

# AssistantAgent knobs (optional; used if set)
ENABLE_RAG=true                 # prepend notes from index
LLM_LOG_PAYLOAD=false           # log prompt/response previews
LLM_LOG_MAX_CHARS=800
LLM_TIMEOUT_S=                  # e.g., 60
SLOW_LLM_WARN_S=20.0
SLOW_RAG_WARN_S=3.0
SLOW_MEM0_WARN_S=1.0

# ---- NEW: tool + mem0 logging knobs ----
TOOL_LOG_ARGS=true
TOOL_LOG_RESULT=true
TOOL_LOG_MAX_CHARS=400
MEM0_LOG_MAX_CHARS=400

# =========================
# MCP / Tools (API keys)
# =========================
MEM0_API_KEY=<MEM0_API_KEY>               # optional; CHANGEME
METAPHOR_API_KEY=<METAPHOR_API_KEY>       # CHANGEME
TAVILY_API_KEY=<TAVILY_API_KEY>           # CHANGEME
SERPER_API_KEY=<SERPER_API_KEY>           # CHANGEME
FIRECRAWL_API_KEY=<FIRECRAWL_API_KEY>     # CHANGEME
ELEVENLABS_API_KEY=<ELEVENLABS_API_KEY>   # CHANGEME
WOLFRAM_ALPHA_APPID=<WOLFRAM_ALPHA_APPID> # CHANGEME

